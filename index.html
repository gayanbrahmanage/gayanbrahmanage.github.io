<resume>
  <personal_information>
    <name>
      <first_name>Gayan</first_name>
      <last_name>Brahmanage</last_name>
    </name>
    <address>
      <street>1-2215, 28 Street SW</street>
      <city>Calgary</city>
      <state>AB</state>
      <country>Canada</country>
    </address>
    <contact>
      <phone>+1 (587) 890 4422</phone>
      <email>gayansampathefac@gmail.com</email>
      <linkedin>gayan-brahmanage</linkedin>
      <github>gayanbrahmanage</github>
      <website>https://gayanbrahmanage.github.io/</website>
    </contact>
  </personal_information>

  <summary>
    <paragraph>Innovative and results-driven Computer Vision &amp; Deep Learning Engineer with over six years of ML-based real-time software development experience. Strong foundation in Bayesian estimation, SLAM, and 3D perception, with proven expertise in designing and optimizing AI-driven vision systems for robotics, autonomous navigation, and intelligent video analytics. Adept at developing high-performance deep learning models for classification, object detection, depth prediction, and 3D reconstruction. Strong command of C++ and Python, enabling the creation of real-time, scalable solutions for complex computational challenges. Recognized for exceptional analytical problem-solving skills, delivering cutting-edge algorithms and robust software architectures that push the boundaries of computer vision and AI.</paragraph>
  </summary>

  <education>
    <degree>
      <degree_name>Doctor of Philosophy (Ph.D.) in Electrical and Computer Engineering</degree_name>
      <university>University Calgary</university>
      <location>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2018-01</start_date>
        <end_date>2024-12</end_date>
      </dates>
      <thesis_title>RGB Predicted Depth Simultaneous Localization and Mapping (SLAM)- Experiments Conducted for Autonomous Driving Applications.</thesis_title>
      <links>
          <link>https://github.com/gayanbrahmanage/i3DSLAM_beta/tree/main</link>
          <link>https://github.com/gayanbrahmanage/StereoDataRecorder</link>
          <link>https://www.youtube.com/watch?v=bEIRYtwlFlk&list=PLBAdHj6DY17oRUEnhwUAhfBmTwiGvwDY5&ab_channel=GayanBrahmanage</link>
      </links>
    </degree>
    <degree>
      <degree_name>Master of Science (MS.c.) in Electrical and Computer Engineering</degree_name>
      <university>University Calgary</university>
      <location>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2015-01</start_date>
        <end_date>2017-12</end_date>
      </dates>
      <gpa>3.96/4.00</gpa>
      <thesis_title>RGB-D SLAM for Autonomous Robots.</thesis_title>
    </degree>
    <degree>
      <degree_name>Bachelor of Science (BS.c. Hons) in Engineering-Robotics.</degree_name>
      <university>University Moratuwa</university>
      <location>
        <country>Sri Lanka</country>
      </location>
      <dates>
        <start_date>2008-01</start_date>
        <end_date>2013-12</end_date>
      </dates>
      <classification>Second-class Upper Division</classification>
    </degree>
  </education>

  <research_experience>
    <experience>
      <title>Graduate Research Assistant</title>
      <department>Department of Electrical and Computer Engineering</department>
      <university>University of Calgary</university>
      <location>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2018-01</start_date>
        <end_date>2024-12</end_date>
      </dates>
      <description>Conducted research works on Real-Time SLAM and deep-learning based depth prediction from monocular (single) images for autonomous driving applications.</description>
    </experience>
    <experience>
      <title>Graduate Research Assistant</title>
      <department>Department of Electrical and Computer Engineering</department>
      <university>University of Calgary</university>
      <location>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2015-01</start_date>
        <end_date>2018-12</end_date>
      </dates>
      <description>Conducted research works on Real-Time SLAM using RGB-D cameras in small scale environments.</description>
    </experience>
  </research_experience>

  <skills>
    <skill>
      <category>Programming</category>
      <description>With over 15 years of experience in computer programming. Expertise in languages such as C, C++, and Python, as well as low level microcontroller programming including RTOS and CUDA is possessed. Proficiency in OpenCV, OpenGL, and GTK for graphical user interface design is included in the skill set. Additionally, extensive experience with a diverse range of sensor systems is held, and real-time software processing pipelines tailored for sensor-based software applications have been developed.</description>
    </skill>
    <skill>
      <category>Software</category>
      <description>Robot operating system (ROS), PyTorch, TensorFlow, Keras Functional API, and Matlab.</description>
    </skill>
    <skill>
      <category>Deep-learning</category>
      <description>Convolutional Neural Networks (CNNs), Vision Transformers, Supervised, Unsupervised learning, Classification,  Object detection and Tracking, Model quantization, Pruning, Multi-task learning models.</description>
    </skill>
    <skill>
      <category>Areas of Expert</category>
      <description>Estimation theory (Multi-variate distributions, EKF, Particle Filter, Rao-Blackwellized Particle Filter, Least-Squares, factor graph, IMU-GPS-Camera-PointCloud fusion ), SLAM and 3D-Perception (sensors and deep-learning based generative 3D), 3D Geometry, Linear Algebra (Eigen) and Standard computer-vision techniques (Monocular and Stereo vision, multi-view geometry, optical-flow, image registration).</description>
    </skill>
    <skill>
      <category>Other</category>
      <description>Matlab simulation and low level C++  implementation of Estimation algorithms in synchronous parallel threads  for real-time operations, strong problem-solving, analytical and development skills.</description>
    </skill>
  </skills>

  <work_experience>
    <job>
      <title>Computer Vision Developer</title>
      <company>Complex System Inc.</company>
      <location>
        <city>Calgary</city>
        <state>AB</state>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2018-01</start_date>
        <end_date>Present</end_date>
      </dates>
      <description>I lead the development of select video analytics projects, leveraging advanced computer vision techniques to deliver solutions tailored to customer-specific requirements, timelines, and deliverable. My role encompasses the design and implementation of real-time software systems, ensuring that solutions are optimized for hardware constraints, performance, and robustness. For the projects under my leadership, I manage the full development lifecycle, from research and development to deployment, ensuring that each solution aligns with both the customer’s objectives and technical specifications while driving continuous improvement in efficiency, functionality, and reliability.

      -Predictive 3D Collision Avoidance System for Canada Robot Arm on ISS- Led the development of a predictive 3D collision avoidance system for the Canada Robot Arm on the International Space Station (ISS). My primary responsibility was developing a metric-scale depth prediction model from single images, utilizing existing ISS cameras to achieve precise 3D clearance volume estimation for robotic arm operations, complemented by EKF-based object tracking capabilities. Additionally, I designed and implemented real-time software integrating the depth prediction model, object detection, 3D tracking modules, and a 3D GUI.

      To meet the stringent requirements set by the Canadian Space Agency, the depth prediction model was optimized for real-time performance on space edge devices, achieving 5 FPS. This optimization involved reducing model precision from 32-bit to 16-bit floating point (FP16) using TensorRT, applying structured pruning to minimize computational complexity, and leveraging CUDA and TensorRT for hardware-specific acceleration. The model was exported to a lightweight ONNX format for efficient deployment and faster inference. Model distillation further enhanced efficiency by training a compact student model that retained the accuracy of a larger 110M parameter teacher model. These advancements ensured resource-efficient, high-accuracy operation, enabling safe and effective robotic arm maneuvers in space.

      Software Tool for Ship Tracking Using WorldView-3 and AIS Data - I involved in a development of a software tool for real-time ship tracking by integrating WorldView-3 satellite imagery with AIS data. The system utilized Mask R-CNN for ship detection in satellite images, and the AIS tracking information was fused with Extended Kalman Filter (EKF) to generate smooth and continuous ship trajectories over extended periods. The project addressed several key challenges, including the discontinuities in AIS data, high levels of noise, and varying sampling rates, by implementing robust data fusion and filtering techniques. The tool significantly improved the accuracy of ship tracking and trajectory prediction, ensuring reliable maritime monitoring despite inconsistencies in AIS data.
      </description>
    </job>
    <job>
      <title>Research Engineer</title>
      <company>Arthur C Clarke Institute for Modern Technology</company>
      <location>
        <city>Moratuwa</city>
        <country>Sri Lanka</country>
      </location>
      <dates>
        <start_date>2015-01</start_date>
        <end_date>2015-12</end_date>
      </dates>
      <description>I involved in the reverse engineering of a low-cost CCD tea color sorter designed to meet the specific needs of the Sri Lankan tea industry. My role involved analyzing and replicating existing systems while improving them to create an affordable solution. I was responsible for the complete mechanical and electronic design, including the development of the sorting mechanism and the integration of CCD technology for precise color detection. Additionally, I supervised the manufacturing process to ensure adherence to quality standards and efficient production of the final product.</description>
    </job>
    <job>
      <title>Electronics Maintenance Engineer</title>
      <company>S-Lon Lanka Private Limited</company>
      <location>
        <city>Pannala</city>
        <country>Sri Lanka</country>
      </location>
      <dates>
        <start_date>2014-01</start_date>
        <end_date>2015-12</end_date>
      </dates>
      <description>The responsibility for the electrical maintenance of the PVC pipe manufacturing machines was undertaken.</description>
    </job>
    <job>
      <title>Lab Instructor</title>
      <company>Faculty of Engineering, University of Moratuwa</company>
      <location>
        <city>Moratuwa</city>
        <country>Sri Lanka</country>
      </location>
      <dates>
        <start_date>2013-01</start_date>
        <end_date>2014-12</end_date>
      </dates>
      <description>The responsibility of conducting laboratory classes and tutorials for undergraduate students was undertaken.</description>
    </job>
    <job>
      <title>Electronics circuits and PCB developer</title>
      <company>Power Migration Partners UK Ltd (Part time)</company>
      <location>
        <city>Edinburgh</city>
        <country>United Kingdom</country>
      </location>
      <dates>
        <start_date>2013-01</start_date>
        <end_date>2014-12</end_date>
      </dates>
      <description>The responsibility of reverse engineering, simulating and developing low-power circuits for embedded hardware and PCB development and evaluation was undertaken.</description>
    </job>
  </work_experience>

  <projects>
    <project>
      <title>Receptionist Robot- Humanoid Robot with a Mobile Base</title>
      <company>University of Calgary</company>
      <location>
        <city>Calgary</city>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2022-01</start_date>
        <end_date>Present</end_date>
      </dates>
      <description>Leading the development of a Receptionist Robot- Humanoid Robot with a Mobile Base designed to assist and interact with visitors at the City of Calgary premises. As the lead developer and designer, responsibilities include the complete design and assembly of the robot's mechanical and electronic systems, alongside the development of its controller software. The top-level software architecture is developed using ROS, emphasizing modular, node-based hardware software design for enhanced safety, efficiency, and scalability with performance, fault tolerance, and robustness in diverse operational scenarios. The actuators are managed via a multi-level CAN Bus network, while sensors are interfaced with microcontrollers through SPI and I2C protocols. Custom firmware has been developed for the robot’s lower-level actuators and sensors to ensure optimal performance and integration. Collaborative efforts with teams from the University of Calgary and the City of Calgary are focused on incorporating advanced hardware, software, and natural language processing (NLP) capabilities to create an interactive and engaging public experience. Open-source  NLP, object detection, in-house RGB-D SLAM and 3D reconstruction packages have been customized and optimized to align with the specific hardware requirements. </description>
      <links>
          <link>https://github.com/gayanbrahmanage/kella-v1.0</link>
      </links>
    </project>
    <project>
      <title>monocular depth prediction</title>
      <company>University of Calgary</company>
      <location>
        <city>Calgary</city>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2021-01</start_date>
        <end_date>2023-12</end_date>
      </dates>
      <description>Developed a Python-TensorFlow implementation for monocular depth prediction, integrating Ph.D. research findings on single-image depth estimation. This implementation introduces the Dual Convolution Vision Transformer (DCViT) for metric-scale depth prediction using unsupervised learning techniques. The project highlights advanced methodologies in depth estimation, leveraging unsupervised learning to achieve accurate metric-scale depth from single images, and builds upon extensive research conducted during my PhD, advancing the field of depth prediction technology.</description>
      <links>
          <link>https://github.com/gayanbrahmanage/i3D-iMAGE-FORMER</link>
      </links>
    </project>
    <project>
      <title>real-time 3D-SLAM software package</title>
      <company>University of Calgary</company>
      <location>
        <city>Calgary</city>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2018-01</start_date>
        <end_date>2023-12</end_date>
      </dates>
      <description>Developed a real-time 3D-SLAM software package using C++ and GTK for Linux environments, integrating advanced findings from my Ph.D. research with current literature. The software features a multi-threaded architecture with dedicated threads for depth prediction, local mapping, sparse bundle adjustment, and loop closing, operating alongside main threads for GUI visualization. It employs a parallel processing structure for loop processing, including key-point detection, matching, and initial motion estimation using RANSAC and EM, ensuring efficient and effective processing. Dynamic resource allocation optimizes thread performance based on available computational resources, significantly advancing the field of real-time 3D-SLAM technology.</description>
      <links>
          <link>https://github.com/gayanbrahmanage/i3D-SLAM</link>
      </links>
    </project>
    <project>
      <title>autonomous driving dataset</title>
      <company>University of Calgary</company>
      <location>
        <city>Calgary</city>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2020-01</start_date>
        <end_date>2021-12</end_date>
      </dates>
      <description>A project was undertaken to create an autonomous driving dataset as a part of my Ph.D. research work. A custom-developed wide-baseline stereo camera system was devised for this purpose. The sensor was mounted on a car and gathered stereo data in diverse driving scenarios. An IMU and GPS-based sensor system were integrated to obtain ground-truth motion tracking. Additionally, real-time software was developed to record synchronous stereo frames and ground-truth data at 16 frames per second.</description>
      <links>
          <link>https://github.com/gayanbrahmanage/StereoDataRecorder/tree/main</link>
      </links>
    </project>
    <project>
      <title>Edge-AI experimental platform</title>
      <company>University of Calgary</company>
      <location>
        <city>Calgary</city>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2020-01</start_date>
        <end_date>2021-12</end_date>
      </dates>
      <description>I spearheaded the development of an Edge-AI experimental platform for research in our lab, which included building a hexacopter drone equipped with edge-AI processing hardware, assembled from off-the-shelf components to meet our research needs. The drone featured RGB and monochrome cameras, along with a radar system for enhanced capabilities. Additionally, I developed a GUI-based software tool to control an off-the-shelf mini drone, enabling manual control through a gaming joystick for lab experiments.</description>
      <links>
          <link>https://www.youtube.com/watch?v=44MDj9QGuLo</link>
          <link>https://github.com/gayanbrahmanage/3DEDGE_NANOv1.0</link>
      </links>
    </project>
    <project>
      <title>advanced vision-based people counting and crowd management system</title>
      <company>University of Calgary</company>
      <location>
        <city>Calgary</city>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2018-01</start_date>
        <end_date>2019-12</end_date>
      </dates>
      <description>Developed an advanced vision-based people counting and crowd management system for the City of Calgary, leveraging retrained YOLOv5 on custom datasets to detect individuals in crowded environments with high accuracy, even under occlusions. Integrated YOLO detection with 3D human shape detection from RGB-D cameras, achieving exceptional performance, especially with top-down views. Designed and implemented fully functional software for real-time people detection, tracking, and counting, generating daily crowd statistics for specified areas with a connected multiple camera setup. Successfully conducted pilot testing at the City of Calgary’s municipal building, delivering a robust and practical solution for efficient crowd management and data-driven decision-making. While lab demonstration videos have been published, onsite videos remain unpublished due to privacy policies.</description>
      <links>
          <link>https://www.youtube.com/watch?v=J1E4DUu5WZc</link>
      </links>
    </project>
    <project>
      <title>Robot autonomous navigation software</title>
      <company>University of Calgary</company>
      <location>
        <city>Calgary</city>
        <country>Canada</country>
      </location>
      <dates>
        <start_date>2015-01</start_date>
        <end_date>2017-12</end_date>
      </dates>
      <description>Robot autonomous navigation software, building on my Master’s thesis research in 2D SLAM, path planning, and target tracking, was executed in two phases. I first transitioned the robot’s software from a Windows-based system to the Robot Operating System (ROS) using ROS-Kinetic, adapting the existing sensor acquisition code for integration. In the second phase, I developed control software to handle sensor data, perform target detection, execute path planning, and manage trajectory navigation. Additionally, I integrated a laser scanner and an RGB-D camera into the robot, ensuring their functionality within the ROS-based environment.</description>
      <links>
          <link>https://www.youtube.com/watch?v=dNcI8IKpin0</link>
      </links>
    </project>
  </projects>

  <publications>
    <publication>
      <title>A Dynamic ORB-SLAM for Autonomous Driving</title>
      <authors>Gayan Brahmanage and Henry Leung</authors>
      <journal>IEEE Transactions on Vehicular Technology</journal>
      <status>Review completed</status>
    </publication>
    <publication>
      <title>A Convolution Vision Transformer for Monocular Depth Estimation with Learned Feedback Attention</title>
      <authors>Gayan Brahmanage and Henry Leung</authors>
      <journal>IEEE Transactions on Image Processing</journal>
      <status>Under review</status>
    </publication>
    <publication>
      <title>RGB predicted depth simultaneous localization and mapping (SLAM) for outdoor environment</title>
      <authors>Brahmanage, G. S.</authors>
      <journal>Doctoral thesis, University of Calgary, Calgary, Canada</journal>
      <year>2024</year>
    </publication>
    <publication>
      <title>Outdoor RGB-D Mapping Using Intel-RealSense</title>
      <authors>G. Brahmanage and H. Leung</authors>
      <journal>2019 IEEE SENSORS, Montreal, QC, Canada</journal>
      <year>2019</year>
      <doi>10.1109 SENSORS43011.2019.8956916</doi>
    </publication>
    <publication>
      <title>Building 2D Maps with Integrated 3D and Visual Information using Kinect Sensor</title>
      <authors>G. Brahmanage and H. Leung</authors>
      <journal>2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS), Taipei, Taiwan</journal>
      <year>2019</year>
      <doi>10.1109/ICPHYS.2019.8780288</doi>
    </publication>
    <publication>
      <title>RGB-PD SLAM: Scale Consistent Monocular SLAM using Predicted Depth</title>
      <authors>Nawal Mehbas, Brahmanage Gayan and Leung Henry.</authors>
      <journal>359-360. 10.1109/ICCE-Taiwan62264.2024.10674415</journal>
      <year>2024</year>
    </publication>
  </publications>

  <awards>
    <award>
      <name>Ph.D. thesis was nominated for Governor General’s Gold Medal and Chancellor’s Graduate Medal Competition</name>
      <organization>Electrical and Computer Engineering-University of Calgary</organization>
      <year>2024</year>
    </award>
    <award>
      <name>Research Productivity Award</name>
      <organization>Electrical and Computer Engineering-University of Calgary</organization>
      <year>2020</year>
    </award>
    <award>
      <name>Alberta Graduate Excellence Scholarship (AGES)</name>
      <organization>Electrical and Computer Engineering-University of Calgary</organization>
      <year>2019</year>
    </award>
    <award>
      <name>Research Productivity Award</name>
      <organization>Electrical and Computer Engineering-University of Calgary</organization>
      <year>2018</year>
    </award>
    <award>
      <name>Academic Excellence Award</name>
      <organization>Electrical and Computer Engineering-University of Calgary</organization>
      <year>2017</year>
    </award>
    <award>
      <name>Alberta National Award Scholarship</name>
      <organization>Electrical and Computer Engineering-University of Calgary</organization>
      <year>2017</year>
    </award>
    <award>
      <name>Dean's List</name>
      <organization>Faculty of Engineering-University of Moratuwa</organization>
      <year>2014</year>
    </award>
    <award>
      <name>ABU Robocon 2010</name>
      <description>As one of the three members, I participated in representing Sri Lanka at the "ABU ROBOCON 2010" Asian Pacific International Robot Contest organized by the Asian Broadcasting Unit.</description>
      <year>2010</year>
    </award>
  </awards>

</resume>
